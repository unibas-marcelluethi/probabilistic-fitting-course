---
id: linearregression
title: Online Course
---
# Bayesian linear regression

In the previous video we have already seen how to update our beliefs in a Bayesian setting.
We have factored the joint distribution $$p(l,s)$$ as $$p(l, s) = p(s|l)p(l)$$ and 
then defined the prior distribution over the length and the likelihood function. 
Using Bayes rule it was then simple to compute the posterior belief of $$p(l | s)$$. 
A crucial step in our modelling was the likelihood function. We just assumed that
$$p(l | s) = N(l, 9)$$, i.e. that the value we observe for the length is also the 
mean for the span. However, usually, we don't know such a relationship directly and would 
like to estimate this from data. In this article we show how Bayesian inference 
applies in this setting. 

### Bayesian regression of 2D hands

A widely used model model is the linear regression model.
In this model, the (mean-free) length $$l$$ is assumed to be a linear function of the  span $$s$$, perturbed by Gaussian noise $$\epsilon \sim N(0, \sigma^2)$$: 
$$
l = a \cdot (s - \overline{s}) + b + \epsilon.
$$
Here the slope $$a$$ and intercept $$b$$ and $$\sigma^2$$ are parameters of our model. $$\overline{s}$$ denotes the mean of the data. Subtracting the mean makes the intercept parameter more easy to interpret, as 
we know that the intercept is the length when the span $$s$$ is the mean hand.
 
These modelling assumptions lead to the following likelihood function:
$$
p( l | a, b, \sigma^2, s) = N(a \cdot (s - \overline{s}) + b, \sigma^2).
$$
Note that if we knew the parameters, we would be back at the case that we discussed in the previous video and could predict likely values of $$l$$ for a given $$s$$.
Although we don't know the values of these parameters exactly, we have some prior beliefs 
about them. I believe, for example, that the span is approximately the same as the length, which leads me to define the following prior distributions:
$$
\begin{array}{l}
a \sim N(1, 0.1) \\
b \sim N(0, 2) \\
\sigma^2 \sim logNormal(0, 0.25)
\end{array}
$$
Note that these were just my prior beliefs. Somebody else could assign different distributions. 

Given observations $$(s_i, l_i), i = 1, \ldots, n$$ we can use Bayes rule to infer the posterior distribution $$p(a, b | s_i, l_i)$$:
$$
p(a, b, \sigma^2 | s_i, l_i) = \prod_{i=1}^n \frac{p(a)p(b)p(\sigma^2) p(l_i | a, b, \sigma^2, s_i)}{\int_{a} \int_{b} \int_{\sigma^2} p(a)p(b)p(\sigma^2) p(l_i | a, b, \sigma^2, s_i) \, da \, db \, d\sigma^2}
$$
This posterior distribution reflects our knowledge about the parameters after we have seen the data. 

While it is difficult to compute the posterior distribution analytically, writing computer 
programs that compute an approximate posterior distribution is relatively simple. We will learn how to dow it in the next week. Here we just show report the result of the computation. 
Figure 2 shows the distribution of possible parameter values of a and b after we have observed the  data shown in Figure 1.  

### Bayesian regression for 3D shape modelling

Assume now that we have a deformation model in low-rank form, as discussed in 
week 5 of the FutureLearn course: 
$$
u[\alpha](x) \sim \mu(x) + \sum_{i=1}^r \alpha_i \sqrt{\lambda_i} \varphi_i(x).
$$
Recall that if $$(\lambda_i, \varphi_i)$$ are the eigenfunction/eigenvalue pair of the covariance operator associated with the Gaussian process $$GP(\mu, k)$$. If 
$$\alpha_i \sim N(0, 1)$$, then $$u$$ is distributed according to $$GP(\mu, k)$$.
We assumed that any target shape can be generated by applying 
a deformation field with the appropriate coefficients $\alpha$. More precisely,
the correct deformation that relates a point $$x_i^T$$ of a given target surface $$\Gamma_T$$  with the corresponding point on the reference surface $$x_i^T$$ is given by 
$$
x_i^T = x_i^R + u[\alpha](x_i^R) + \epsilon = x_i^R + \mu(x_i^R) + \sum_{i=1}^n \alpha_i \sqrt{\lambda_i} \varphi_i(x_i^R) + \epsilon
$$
where
$$
\epsilon \sim N(0, \sigma^2 I_{3 \times 3})
$$ 
is Gaussian noise. 
We recognize that this is another linear regression model, as it is linear in the parameters $\alpha_i$. 
The corresponding likelihood function is
$$
p(x_i^T | \alpha, x_i^R, \sigma^2) = N(u(x_i)[\alpha], \sigma^2) = N(\mu(x) + \sum_{i=1}^r \alpha_i \sqrt{\lambda_i} \varphi_i(x), \sigma^2).
$$
The prior on the shpae parameters is, by assumption above, 
$$
p(\alpha) = N(0, I_{r \times r}).
$$
For the noise term, we use a log-normal distribution as before
$$
p(\sigma^2) = N(0, 0.25)
$$


It turns out that this is exactly the setting of Gaussian process regression that we discussed in the previous course. However, with this formulation, we hope that it is now easy to see how we can generalize it to include more parameters, change the assumption about the noise or form or the likelihood function, or even relax the assumption of 
strict correspondence. In all the cases, a closed form solution will not be available anymore 
and we will have to resort to computational methods. 